📘 주성분분석(PCA)이란?
✅ 목적
차원 축소(Dimensionality Reduction)
서로 상관된 변수들을 직교한(uncorrelated) 축으로 변환
데이터의 분산을 최대한 보존하는 선형 조합을 찾는 것

✅ 주요 개념
주성분(Principal Components): 원래 변수들의 선형 조합
분산(Variance): 각 주성분이 설명하는 데이터의 정보량
고유값 분해(Eigendecomposition): 공분산 행렬을 고유값과 고유벡터로 분해

🧮 R에서 PCA 수행 절차
1️⃣ 데이터 준비 및 정규화
data(iris)
df <- iris[, -5]  #종(Species) 변수 제외
df_scaled <- scale(df) #정규화 (권장: 각 변수의 단위가 다를 경우)

2️⃣ PCA 수행
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE) #prcomp() 함수는 특이값 분해(SVD) 를 사용하여 주성분분석을 수행

3️⃣ 결과 확인
summary(pca_result)
pca_result$sdev^2 / sum(pca_result$sdev^2) #주성분 별 분산(설명력)
pca_result$rotation #로딩값 (각 변수와 주성분 간의 관계)
head(pca_result$x) #주성분 점수 (새로운 축 기준의 데이터)

4️⃣ 시각화
plot(pca_result, type = "l") #Scree plot (설명력 시각화)
set.seed(1)
kmeans_result <- kmeans(selected_pcs, centers = 3)
plot(selected_pcs, col = kmeans_result$cluster)
✔️ 색 기준: 군집 결과(cluster)
kmeans() 결과의 cluster 값을 색깔로 사용 → PCA 축 위에서 K-means가 자동으로 분류한 그룹
즉, 정답 없이 비지도학습(Unsupervised Learning) 으로 군집화한 것
목적: "주성분으로 비슷한 데이터끼리 잘 뭉치는가?" 확인

plot(pca_result$x[,1], pca_result$x[,2],
     col = iris$Species,
     xlab = "PC1", ylab = "PC2", main = "PCA 2D Plot")
✔️ 색 기준: 정답(label)
iris$Species를 색깔로 사용 → 실제 품종(Species) 에 따라 색을 다르게 표현
즉, **정답이 있는 지도학습(Supervised Learning)**의 시각화
목적: "주성분으로 진짜 클래스를 얼마나 잘 구분할 수 있나?" 확인
